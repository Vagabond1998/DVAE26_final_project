{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b854ce2c-18d8-4979-b2da-5f6e0ced97f8",
   "metadata": {},
   "source": [
    "# Final Project - MNIST Image Classification\n",
    "This notebook validates the full image classification pipeline developed for the final project.\n",
    "It demonstrates all major steps of the solution, including data ingestion, preprocessing,\n",
    "model training, and evaluation.\n",
    "\n",
    "All functionality shown in this notebook relies on the modular Python code implemented\n",
    "in the project repository. The purpose of the notebook is to provide an executable and\n",
    "reproducible validation of the complete system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90555d7e-aed5-4e4a-af15-600ec42658cd",
   "metadata": {},
   "source": [
    "## Task - Image Reconition\n",
    "I have chosen the image recognition task for this final project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54e943-3dad-4c55-b267-5f766d00f8f4",
   "metadata": {},
   "source": [
    "### Environment and Setup\n",
    "This section verifies the execution environment used for validation.\n",
    "The project is implemented in PyTorch and supports both CPU and GPU execution.\n",
    "If a CUDA-enabled GPU is available, it will be used automatically during training\n",
    "and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a43d7c-bc10-4ad3-97d8-8202e3b1e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/jarla/Documents/DVAE26/DVAE26_final_project\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 980 Ti\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "cdir = os.chdir(ROOT)\n",
    "\n",
    "print(ROOT)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac3a97-2874-48ac-9fe2-4c9ab21922bd",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "The MNIST dataset is not stored directly in the repository.\n",
    "Instead, a dedicated ingestion step is used to download and prepare the dataset\n",
    "in a reproducible manner.\n",
    "\n",
    "This approach ensures that the repository remains lightweight and that the dataset\n",
    "can be recreated on any machine by executing the ingestion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0835d1f-6a00-4ee3-b680-a942724a22f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/c/Users/jarla/Documents/DVAE26/DVAE26_final_project/data')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from steps.ingest import download_mnist\n",
    "data_dir = ROOT / \"data\"\n",
    "download_mnist(data_dir=str(data_dir))\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18587dc8-2620-45b5-9e90-a11d086b4fc6",
   "metadata": {},
   "source": [
    "### Dataset Inspection and Sanity Checks\n",
    "Before training, the dataset is inspected to verify its basic properties.\n",
    "This includes checking the number of samples in the training and test sets,\n",
    "the image resolution, and the available class labels.\n",
    "\n",
    "These checks help confirm that the dataset is correctly loaded and matches\n",
    "the expected MNIST specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5af7303-d7d8-4dbd-b4b3-487615049875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 60000\n",
      "Test samples: 10000\n",
      "Image size: (28, 28)\n",
      "Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_raw = datasets.MNIST(root=str(data_dir), train=True, download=False)\n",
    "test_raw = datasets.MNIST(root=str(data_dir), train=False, download=False)\n",
    "\n",
    "print(\"Train samples:\", len(train_raw))\n",
    "print(\"Test samples:\", len(test_raw))\n",
    "print(\"Image size:\", train_raw[0][0].size)\n",
    "print(\"Unique labels:\", sorted(set(train_raw.targets.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2896fa6-2cdd-4258-9621-fb30e3d5c7e3",
   "metadata": {},
   "source": [
    "### Preprocessing and Augmentation\n",
    "The data preprocessing pipeline consists of two conceptually different steps:\n",
    "\n",
    "1. **Data transformation**, which converts raw images into normalized tensors\n",
    "   suitable for neural network training.\n",
    "2. **Data augmentation**, which introduces controlled randomness during training\n",
    "   to improve model generalization.\n",
    "\n",
    "Augmentation is applied only to the training data, while the test data uses\n",
    "deterministic preprocessing to preserve evaluation integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64df6269-c074-46b8-af62-bf98aae9c2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.1307,), std=(0.3081,))\n",
       " ),\n",
       " Compose(\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.1307,), std=(0.3081,))\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from steps.transform import get_transforms\n",
    "\n",
    "train_tf = get_transforms(train=True)\n",
    "test_tf = get_transforms(train=False)\n",
    "\n",
    "train_tf, test_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc67caf-1059-40bc-8767-d9823eaa7256",
   "metadata": {},
   "source": [
    "### Visual Validation of Augmentation\n",
    "To verify that data augmentation is applied correctly, the same training image\n",
    "is passed through the preprocessing pipeline multiple times.\n",
    "Due to the stochastic nature of the augmentation step, each output may differ\n",
    "slightly while preserving the original label.\n",
    "\n",
    "This visual inspection confirms that augmentation is active and functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01585b1-4425-4c83-a5f1-bb225487c05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = [train_tf(train_raw[0][0]) for _ in range(6)]\n",
    "label = train_raw[0][1]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for i, img in enumerate(imgs):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(f\"Augmented samples (label={label})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d2dba-87ed-4da2-b3b1-949feb2b009c",
   "metadata": {},
   "source": [
    "### DataLoaders and Batch Shapes\n",
    "\n",
    "The dataset is loaded using PyTorch DataLoaders, which enable efficient batching\n",
    "and shuffling during training.\n",
    "\n",
    "In this step, a single batch is inspected to verify:\n",
    "- the batch size\n",
    "- the tensor shape of the input images\n",
    "- the corresponding label tensor shape\n",
    "\n",
    "The batch dimension (B) represents the number of images processed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7011ed-157e-48e1-bc25-a5d181226c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from src.data import get_dataloaders\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(\n",
    "    data_dir=str(data_dir),\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4946590-cb9b-44e5-8a1b-e217f60677f2",
   "metadata": {},
   "source": [
    "### Model Instantiation and Forward Pass\n",
    "\n",
    "The convolutional neural network (CNN) used for this project is instantiated\n",
    "and evaluated on a sample batch.\n",
    "\n",
    "A forward pass is performed to verify that:\n",
    "- the model accepts inputs with the expected shape\n",
    "- the output consists of logits for the 10 MNIST classes\n",
    "\n",
    "This step confirms that the model architecture is correctly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c7d337-2f7c-4175-add7-9ab2117799f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "from src.model import MNISTCNN\n",
    "\n",
    "model = MNISTCNN()\n",
    "logits = model(images)\n",
    "\n",
    "print(\"Model output shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0c669-6581-46e7-9021-9fe664fa9e02",
   "metadata": {},
   "source": [
    "### Training Procedure\n",
    "The model is trained using the predefined training pipeline.\n",
    "For validation purposes, the number of epochs is kept small to ensure\n",
    "reasonable execution time within the notebook.\n",
    "\n",
    "The full training procedure, including loss computation and parameter updates,\n",
    "is implemented in the modular training step and reused here without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdffe7b9-e61b-4f21-abe5-c79afce33129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:16<00:00, 56.47it/s, acc=0.9563, loss=0.1434]\n",
      "Epoch 2/15: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:13<00:00, 70.36it/s, acc=0.9839, loss=0.0512]\n",
      "Epoch 3/15: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:13<00:00, 71.38it/s, acc=0.9881, loss=0.0372]\n",
      "Epoch 4/15:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 780/938 [00:11<00:02, 68.72it/s, acc=0.9904, loss=0.0285]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msteps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m----> 3\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m weights_path\n",
      "File \u001b[0;32m/mnt/c/Users/jarla/Documents/DVAE26/DVAE26_final_project/steps/train.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_dir, artifacts_dir, epochs, batch_size, lr, num_workers, seed, save_best_only)\u001b[0m\n\u001b[1;32m     99\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/jarla/Documents/DVAE26/DVAE26_final_project/src/model.py:22\u001b[0m, in \u001b[0;36mMNISTCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# (B, 32, 14, 14)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)             \u001b[38;5;66;03m# (B, 64, 14, 14)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)                      \u001b[38;5;66;03m# (B, 64*14*14)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))               \u001b[38;5;66;03m# (B, 128)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DVAE26/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from steps.train import train\n",
    "\n",
    "weights_path = train(\n",
    "    data_dir=str(data_dir),\n",
    "    artifacts_dir=str(ROOT / \"artifacts\"),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    num_workers=2,\n",
    ")\n",
    "weights_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5489b-62d7-4e33-a2e7-8a30d3fb14fc",
   "metadata": {},
   "source": [
    "### Evaluation and Metrics\n",
    "\n",
    "After training, the model is evaluated on the test dataset.\n",
    "The evaluation step computes standard classification metrics, including:\n",
    "\n",
    "- Accuracy\n",
    "- Macro-averaged precision\n",
    "- Macro-averaged recall\n",
    "\n",
    "In addition, a confusion matrix is generated to provide a more detailed\n",
    "view of the model’s classification behavior across all digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba777f1-5159-4cb5-b042-d1fb49f0069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.evaluate import evaluate\n",
    "\n",
    "artifacts_dir = ROOT / \"artifacts\"\n",
    "metrics_path = evaluate(\n",
    "    data_dir=str(ROOT / \"data\"),\n",
    "    artifacts_dir=str(artifacts_dir),\n",
    "    weights_path=str(artifacts_dir / \"mnist_cnn.pt\"),\n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    ")\n",
    "print(\"Metrics saved to:\", metrics_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5489dc-bb31-441e-8902-5d730be59ffb",
   "metadata": {},
   "source": [
    "### Results Visualization\n",
    "The computed evaluation metrics are loaded and displayed in structured form.\n",
    "The confusion matrix is visualized to highlight class-specific performance\n",
    "and potential misclassification patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5ef85-a6a1-4218-8d66-4ae3ddb63769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "with open(metrics_path) as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21b5c9-d3df-4fa2-b4f7-9adfea5a074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(artifacts_dir / \"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9f1dd-8606-4b36-a89d-bc8690ac9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting import plot_training_history\n",
    "\n",
    "hist = json.loads(Path(ROOT / \"artifacts\" / \"train_history.json\").read_text(encoding=\"utf-8\"))\n",
    "plot_training_history(hist[\"history\"], out_dir=ROOT / \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd632e5-17a5-4de5-843f-ae237bd1b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best epoch:\", hist[\"best_epoch\"])\n",
    "print(\"Best test acc:\", hist[\"best_test_acc\"])\n",
    "print(\"Best test loss:\", hist[\"best_test_loss\"])\n",
    "print(\"Config:\", hist[\"config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c19bc3-12d6-4792-8708-4e351183b6da",
   "metadata": {},
   "source": [
    "### Validation Summary\n",
    "This notebook has validated the complete MNIST image classification pipeline.\n",
    "All stages of the system, from data ingestion to evaluation, were executed successfully.\n",
    "\n",
    "Key aspects such as dataset sizes, tensor shapes, model interfaces, and evaluation\n",
    "metrics were explicitly verified, providing confidence in the correctness and\n",
    "reproducibility of the implemented solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf03334-2116-4cce-a048-c4018cd396b4",
   "metadata": {},
   "source": [
    "## Deployment To Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf3ab9-bfd5-4492-b0b1-a610f251161d",
   "metadata": {},
   "source": [
    "### Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66faec1a-f799-4d02-ae5b-a71e32bf3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "#checkout git\n",
    "#login to huggingface, setting, access token, generate a new token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d257-dab6-41ae-a118-76d5c02b9e81",
   "metadata": {},
   "source": [
    "### Preparation for Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c888bac-d299-4016-a3ab-74cd9e271819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "HF_DIR = Path(\"hf_model\")\n",
    "HF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy trained weights\n",
    "shutil.copy(\"artifacts/mnist_cnn.pt\", HF_DIR / \"mnist_cnn.pt\")\n",
    "\n",
    "# Copy model architecture\n",
    "shutil.copy(\"src/model.py\", HF_DIR / \"model.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760b741-b3ed-4535-b3a4-28295dedfeeb",
   "metadata": {},
   "source": [
    "### Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a42d4-c991-491d-ae6e-610b416f0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "USERNAME = \"Vagabond98\"\n",
    "REPO_ID = f\"{USERNAME}/dvae26-final-project\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "api.create_repo(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    private=False,\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=REPO_ID,\n",
    "    folder_path=\"hf_model\",\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(\"Uploaded to:\", f\"https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55c654-b8ec-49f8-b9b8-9f6dc85fbec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DVAE26",
   "language": "python",
   "name": "dvae26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
